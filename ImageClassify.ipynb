{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee41b1e8-8276-4f09-a1d3-33918fedc1f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/app-root/lib/python3.9/site-packages (4.45.1)\n",
      "Requirement already satisfied: tensorflow in /opt/app-root/lib/python3.9/site-packages (2.15.1)\n",
      "Requirement already satisfied: datasets in /opt/app-root/lib/python3.9/site-packages (3.0.1)\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.6.17.tar.gz (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib/python3.9/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/app-root/lib/python3.9/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (1.66.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (2.15.2)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (69.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in /opt/app-root/lib/python3.9/site-packages (from datasets) (2024.6.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/app-root/lib/python3.9/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: multiprocess in /opt/app-root/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib/python3.9/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: aiohttp in /opt/app-root/lib/python3.9/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: xxhash in /opt/app-root/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /opt/app-root/lib/python3.9/site-packages (from kaggle) (2024.7.4)\n",
      "Requirement already satisfied: python-dateutil in /opt/app-root/lib/python3.9/site-packages (from kaggle) (2.9.0.post0)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: urllib3 in /opt/app-root/lib/python3.9/site-packages (from kaggle) (1.26.19)\n",
      "Requirement already satisfied: bleach in /opt/app-root/lib/python3.9/site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/app-root/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.34.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
      "Requirement already satisfied: webencodings in /opt/app-root/lib/python3.9/site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/app-root/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/app-root/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/app-root/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/app-root/lib/python3.9/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/app-root/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow) (8.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/app-root/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/app-root/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow) (3.20.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/app-root/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/app-root/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.6.17-py3-none-any.whl size=105786 sha256=80ccc236e00f1c706821c91fe9c23d8777e9277d76dfe21cd0cae70ea5aabeb3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-13_vra4d/wheels/2b/af/a9/70bffa2773af622d2ebea9c8d407720b86e67bd40c465bf837\n",
      "Successfully built kaggle\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle\n",
      "Successfully installed kaggle-1.6.17 python-slugify-8.0.4 text-unidecode-1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers tensorflow datasets kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c6a53be-cd96-4dec-9d28-fd21fdfc9925",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/app-root/src/.kaggle:\n",
      "total 12\n",
      "drwxr-sr-x.  2 1006090000 1006090000 4096 Oct  5 05:34 .\n",
      "drwxrwsr-x. 18 root       1006090000 4096 Oct  5 07:04 ..\n",
      "-rw-------.  1 1006090000 1006090000   69 Oct  5 05:34 kaggle.json\n"
     ]
    }
   ],
   "source": [
    "!if [ ! -e ~/.kaggle/kaggle.json ]; then mkdir -p ~/.kaggle; cp ~/kaggle.json ~/.kaggle; echo \"done\"; fi\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!ls -alR ~/.kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de4e6ef-77ad-4ddd-82c8-feba8c36caeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
      "License(s): other\n",
      "Downloading chest-xray-pneumonia.zip to chest_xray\n",
      "100%|██████████████████████████████████████▉| 2.29G/2.29G [00:16<00:00, 114MB/s]\n",
      "100%|███████████████████████████████████████| 2.29G/2.29G [00:16<00:00, 151MB/s]\n"
     ]
    }
   ],
   "source": [
    "# https://nihcc.app.box.com/v/ChestXray-NIHCC\n",
    "\n",
    "#kaggle datasets list\n",
    "\n",
    "!kaggle datasets download paultimothymooney/chest-xray-pneumonia -p chest_xray\n",
    "!unzip -q chest_xray/*.zip \n",
    "!rm -rf chest_xray/__MACOSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900ee0b7-00b2-4a15-83e8-47adac5d5a25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "base = tf.keras.applications.VGG19(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=(228,228,3),\n",
    "    pooling=None,\n",
    "    classes=1000,\n",
    "    classifier_activation='softmax'\n",
    ")\n",
    "\n",
    "print('VGG19 Loaded')\n",
    "print(base.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c7e40-f9d7-4158-9641-e543d9731f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pic_file in  np.random.choice(os.listdir(), 3):\n",
    "    img = mpimg.imread(pic_file)\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a6172a-6096-4c4d-9883-9937f56111c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model to output 3D feature maps (height, width, features)\n",
    "altmodel = tf.keras.models.Sequential()\n",
    "\n",
    "# Layer 1\n",
    "altmodel.add(tf.keras.layers.Conv2D(32, (3, 3), input_shape=(228, 228, 3)))\n",
    "altmodel.add(tf.keras.layers.Activation('relu'))\n",
    "altmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 2\n",
    "altmodel.add(tf.keras.layers.Conv2D(32, (3, 3)))\n",
    "altmodel.add(tf.keras.layers.Activation('relu'))\n",
    "altmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Layer 3\n",
    "altmodel.add(tf.keras.layers.Conv2D(64, (3, 3)))\n",
    "altmodel.add(tf.keras.layers.Activation('relu'))\n",
    "altmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Apply flattening function to convert 3D feature maps in to 1D feature vectors\n",
    "altmodel.add(tf.keras.layers.Flatten())  \n",
    "\n",
    "# Add 2 final dense layers to add a classifier to the convolutional base\n",
    "altmodel.add(tf.keras.layers.Dense(64))\n",
    "altmodel.add(tf.keras.layers.Activation('relu'))\n",
    "altmodel.add(tf.keras.layers.Dropout(0.5))\n",
    "altmodel.add(tf.keras.layers.Dense(1))\n",
    "altmodel.add(tf.keras.layers.Activation('sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "altmodel.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "print(altmodel.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6196e251-d9b3-441e-8255-89000ad5af6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chest_xray/val...\n",
      "Listing chest_xray/val/NORMAL\n",
      "Sample NORMAL2-IM-1440-0001.jpeg\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/opt/app-root/src/NORMAL2-IM-1440-0001.jpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 54\u001b[0m\n\u001b[1;32m     52\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(dataset_root, folder)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(path))\n\u001b[0;32m---> 54\u001b[0m num_samples \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifications\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m features, labels \u001b[38;5;241m=\u001b[39m extract_features(path, \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m1000\u001b[39m, num_samples))\n\u001b[1;32m     56\u001b[0m dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m'\u001b[39m][folder] \u001b[38;5;241m=\u001b[39m features\n",
      "Cell \u001b[0;32mIn[39], line 22\u001b[0m, in \u001b[0;36mdataset_size\u001b[0;34m(directory, labels, showSample)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pic_file \u001b[38;5;129;01min\u001b[39;00m  np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(contents, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSample \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pic_file))\n\u001b[0;32m---> 22\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mmatplotlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     plt\u001b[38;5;241m.\u001b[39mimshow(img)\n\u001b[1;32m     24\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/matplotlib/image.py:1525\u001b[0m, in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(parse\u001b[38;5;241m.\u001b[39murlparse(fname)\u001b[38;5;241m.\u001b[39mscheme) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1519\u001b[0m     \u001b[38;5;66;03m# Pillow doesn't handle URLs directly.\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1521\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease open the URL for reading and pass the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1522\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult to Pillow, e.g. with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1523\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1524\u001b[0m         )\n\u001b[0;32m-> 1525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mimg_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m image:\n\u001b[1;32m   1526\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[1;32m   1527\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(image, PIL\u001b[38;5;241m.\u001b[39mPngImagePlugin\u001b[38;5;241m.\u001b[39mPngImageFile) \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m   1528\u001b[0m             pil_to_array(image))\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.9/site-packages/PIL/Image.py:3431\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3428\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[1;32m   3430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[0;32m-> 3431\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3432\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   3433\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/opt/app-root/src/NORMAL2-IM-1440-0001.jpeg'"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "\n",
    "# Specify dataset size\n",
    "batch_size = 16\n",
    "\n",
    "# Specify reference for directory paths\n",
    "dataset_root = \"chest_xray\"\n",
    "classifications = [ \"NORMAL\", \"PNEUMONIA\" ]\n",
    "folders = [ \"val\", \"train\", \"test\" ]\n",
    "\n",
    "#def preprocess_data(examples):\n",
    "def dataset_size(directory, labels, showSample=False):\n",
    "    size = 0 \n",
    "    for label in labels:\n",
    "        path = \"{}/{}\".format(directory, label)\n",
    "        print(\"Listing {}\".format(path))\n",
    "        contents = os.listdir(path)\n",
    "        size += len(contents)\n",
    "        if showSample:\n",
    "            for pic_file in  np.random.choice(contents, 2):\n",
    "                print(\"Sample {}\".format(pic_file))\n",
    "                img = matplotlib.image.imread(\"{}/{}\".format(path,pic_file))\n",
    "                plt.imshow(img)\n",
    "                plt.show()\n",
    "    return size    \n",
    "\n",
    "def extract_features(directory, sample_amount):\n",
    "    features = np.zeros(shape=(sample_amount, 7, 7, 512)) \n",
    "    labels = np.zeros(shape=(sample_amount))\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0/255) \n",
    "    generator = datagen.flow_from_directory(\n",
    "        directory, target_size=(228, 228), \n",
    "        batch_size = batch_size, \n",
    "        class_mode='binary')\n",
    "    \n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = base.predict(inputs_batch)\n",
    "        features[i * batch_size : (i + 1) * batch_size] = features_batch \n",
    "        labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
    "        i = i + 1\n",
    "        if i * batch_size >= sample_amount:\n",
    "            break\n",
    "    return features, labels\n",
    "\n",
    "# Apply extraction function to 3 datasets\n",
    "dataset = {}\n",
    "dataset['features'] = {}\n",
    "dataset['labels'] = {}\n",
    "\n",
    "for folder in folders:\n",
    "    path = \"{}/{}\".format(dataset_root, folder)\n",
    "    print(\"Processing {}...\".format(path))\n",
    "    num_samples = dataset_size(path, classifications, True)\n",
    "    features, labels = extract_features(path, min(1000, num_samples))\n",
    "    dataset['features'][folder] = features\n",
    "    dataset['labels'][folder] = labels\n",
    "\n",
    "# Shape data\n",
    "reshape_y = 7 * 7 * 512\n",
    "for folder in folders:\n",
    "    print(\"Reprocessing {}...\".format(folder))\n",
    "    dataset['features'][folder] = np.reshape(dataset['features'][folder], (len(dataset['features'][folder]), reshape_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f51b8a-6327-4c0f-a42c-cb807910e3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78471b56-02bc-490a-af0a-aac57f81f269",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of replicas: 1\n",
      "2.15.1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#from kaggle_datasets import KaggleDatasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Device:', tpu.master())\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "except:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "print('Number of replicas:', strategy.num_replicas_in_sync)\n",
    "    \n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581d06f-c6eb-4001-9b6a-b4b9b757d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build classifier on top of  VGG19\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Add dense layers on top of VGG19 \n",
    "# 1\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu', input_dim=reshape_y))\n",
    "# 2\n",
    "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile\n",
    "model.compile(optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(train_features, train_labels,\n",
    "                    epochs=20,\n",
    "                    batch_size=16,\n",
    "                    validation_data=(validation_features, validation_labels))\n",
    "\n",
    "# Save VGG19 results\n",
    "model.save('models/model_VGG_01.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
