{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "853a8940-b29e-410a-ac4b-e8e47e5b7230",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/app-root/lib/python3.9/site-packages (4.45.1)\n",
      "Requirement already satisfied: tensorflow in /opt/app-root/lib/python3.9/site-packages (2.15.1)\n",
      "Requirement already satisfied: datasets in /opt/app-root/lib/python3.9/site-packages (3.0.1)\n",
      "Requirement already satisfied: rouge-score in /opt/app-root/lib/python3.9/site-packages (0.1.2)\n",
      "Requirement already satisfied: nltk in /opt/app-root/lib/python3.9/site-packages (3.9.1)\n",
      "Requirement already satisfied: tf_keras in /opt/app-root/lib/python3.9/site-packages (2.15.1)\n",
      "Requirement already satisfied: requests in /opt/app-root/lib/python3.9/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/app-root/lib/python3.9/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.9/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/app-root/lib/python3.9/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: filelock in /opt/app-root/lib/python3.9/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/app-root/lib/python3.9/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/app-root/lib/python3.9/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (1.66.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (2.15.2)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (69.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /opt/app-root/lib/python3.9/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: fsspec[http]<=2024.6.1,>=2023.1.0 in /opt/app-root/lib/python3.9/site-packages (from datasets) (2024.6.1)\n",
      "Requirement already satisfied: pandas in /opt/app-root/lib/python3.9/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/app-root/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: multiprocess in /opt/app-root/lib/python3.9/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /opt/app-root/lib/python3.9/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/app-root/lib/python3.9/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: xxhash in /opt/app-root/lib/python3.9/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: joblib in /opt/app-root/lib/python3.9/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: click in /opt/app-root/lib/python3.9/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/app-root/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/app-root/lib/python3.9/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (3.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/app-root/lib/python3.9/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.34.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/app-root/lib/python3.9/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/app-root/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/app-root/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/app-root/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.5.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/app-root/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/app-root/lib/python3.9/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/app-root/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow) (8.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/app-root/lib/python3.9/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/app-root/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.16,>=2.15->tensorflow) (3.20.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/app-root/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/app-root/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers tensorflow datasets rouge-score nltk tf_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f81ba849-29cb-4e5f-b96a-cb9a2d0afd6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b16d583c-eb80-48cb-a5f4-7bda8bc346c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'translation': {'de': 'Deine Habgier wird noch dein Tod sein.', 'en': \"It's greed that it's gonna be the death of you, 'cause you...\"}}\n",
      "{'translation': {'de': 'Ihre Nachricht: Ich empfehle Erich von Däniken unter http://www.laendleticket.com/laendleticket/de/tickets/erich-von-daniken-feldkirchen-stadtsaal-411302/event.html?tellfriend&first', 'en': 'Your message: Check Erich von Däniken on http://www.laendleticket.com/laendleticket/en/tickets/erich-von-daniken-feldkirchen-stadtsaal-411302/event.html?videos&state'}}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Helsinki-NLP/opus-100 dataset\n",
    "dataset = load_dataset('Helsinki-NLP/opus-100', 'de-en')\n",
    "print(dataset['train'][0])\n",
    "print(dataset['train'][30000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be3f2e6e-6598-406d-b819-093bf97d6b68",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the dataset for input into the model\n",
    "def preprocess_data(examples):\n",
    "    inputs = [f'Translate from English to German: {example[\"en\"]}' for example in examples['translation']]\n",
    "    targets = [example['de'] for example in examples['translation']]\n",
    "\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n",
    "\n",
    "    # Tokenize targets\n",
    "    labels = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    # For decoder inputs\n",
    "    decoder_inputs = tokenizer(targets, max_length=128, truncation=True, padding=\"max_length\", return_tensors=\"tf\")\n",
    "    model_inputs[\"decoder_input_ids\"] = decoder_inputs[\"input_ids\"]\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "train_dataset = dataset['train'].select(range(10000)).map(preprocess_data, batched=True)\n",
    "test_dataset = dataset['test'].select(range(1000)).map(preprocess_data, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask', 'decoder_input_ids'],\n",
    "    label_cols=['labels'],\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    collate_fn=None\n",
    ")\n",
    "\n",
    "test_dataset = test_dataset.to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask', 'decoder_input_ids'],\n",
    "    label_cols=['labels'],\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    collate_fn=None\n",
    ")\n",
    "\n",
    "print(f\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13dde2f7-d3c7-4102-968b-fb5b3c4b7d6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.get_layer(\"shared\").trainable = False\n",
    "model.get_layer(\"encoder\").trainable = False\n",
    "model.get_layer(\"decoder\").trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f35b617-71fa-4ef5-ac09-636d87b69ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer\n",
      "Loss\n",
      "Compiling\n",
      "Training\n",
      "334/469 [====================>.........] - ETA: 57:36 - loss: 42.8713"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "# Compile the model\n",
    "print(f\"Optimizer\")\n",
    "o=tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "\n",
    "print(f\"Loss\")\n",
    "l=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "print(f\"Compiling\")\n",
    "model.compile(optimizer=o, loss=l)\n",
    "\n",
    "# Train the model\n",
    "print(f\"Training\")\n",
    "model.fit(train_dataset, epochs=1)\n",
    "\n",
    "print(f\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5335fb6-f5d6-4849-aaea-55d68d48c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def translate(inputs):\n",
    "    outputs = model.generate(inputs[0][\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Function to calculate ROUGE and BLEU scores\n",
    "def calculate_scores(reference, hypothesis):\n",
    "    # Initialize scorers\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    bleu_smoothing = SmoothingFunction().method4\n",
    "\n",
    "    # Calculate ROUGE scores\n",
    "    rouge_scores = rouge.score(reference, hypothesis)\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    reference_tokens = [nltk.word_tokenize(reference)]\n",
    "    hypothesis_tokens = nltk.word_tokenize(hypothesis)\n",
    "    bleu_score = sentence_bleu(reference_tokens, hypothesis_tokens, smoothing_function=bleu_smoothing)\n",
    "\n",
    "    return rouge_scores, bleu_score\n",
    "\n",
    "# Evaluate translations and calculate scores\n",
    "batch = next(iter(test_dataset))\n",
    "translated_text = translate(batch)\n",
    "reference_text = tokenizer.decode(batch[1][0], skip_special_tokens=True)\n",
    "rouge_scores, bleu_score = calculate_scores(reference_text, translated_text)\n",
    "print(f\"Reference: {reference_text}\")\n",
    "print(f\"Translation: {translated_text}\")\n",
    "print(f\"ROUGE Scores: {rouge_scores}\")\n",
    "print(f\"BLEU Score: {bleu_score}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b998769-9cb1-4058-afc7-11443de616da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
